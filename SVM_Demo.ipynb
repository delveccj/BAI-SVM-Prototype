{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Support Vector Machines - Interactive Demo\n",
    "\n",
    "Welcome to the BAI SVM Prototype! This notebook demonstrates the core concepts of Support Vector Machines.\n",
    "\n",
    "## üìã Learning Objectives:\n",
    "- Understand what \"support vectors\" are and why they matter\n",
    "- See how SVMs find the optimal decision boundary\n",
    "- Experiment with different kernels\n",
    "- Visualize the impact of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üöÄ Libraries loaded successfully!\")\n",
    "print(\"üìä Ready for SVM exploration!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé® Part 1: Creating Sample Data\n",
    "\n",
    "Let's create a simple 2D dataset to visualize SVM concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate linearly separable data\n",
    "n_samples = 100\n",
    "X = np.random.randn(n_samples, 2)\n",
    "y = (X[:, 0] + X[:, 1] > 0).astype(int)\n",
    "\n",
    "# Add some noise to make it more interesting\n",
    "X += np.random.normal(0, 0.1, X.shape)\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X[y==0, 0], X[y==0, 1], c='red', marker='o', s=100, label='Class 0', alpha=0.7)\n",
    "plt.scatter(X[y==1, 0], X[y==1, 1], c='blue', marker='s', s=100, label='Class 1', alpha=0.7)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('üéØ Sample Dataset for SVM')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"üìä Dataset created: {n_samples} samples, 2 features\")\n",
    "print(f\"üî¥ Class 0: {sum(y==0)} samples\")\n",
    "print(f\"üîµ Class 1: {sum(y==1)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Part 2: Linear SVM - Finding the \"Street\"\n",
    "\n",
    "Now let's train a linear SVM and visualize the decision boundary and support vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svm_decision_boundary(X, y, model, title=\"SVM Decision Boundary\"):\n",
    "    \"\"\"Plot SVM decision boundary and support vectors\"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Create a mesh for plotting decision boundary\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Get decision boundary\n",
    "    Z = model.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot decision boundary and margins\n",
    "    plt.contour(xx, yy, Z, colors='black', levels=[-1, 0, 1], alpha=0.5, \n",
    "                linestyles=['--', '-', '--'])\n",
    "    \n",
    "    # Fill the margin area\n",
    "    plt.contourf(xx, yy, Z, levels=[-1, 1], colors=['lightcoral', 'lightblue'], alpha=0.3)\n",
    "    \n",
    "    # Plot data points\n",
    "    plt.scatter(X[y==0, 0], X[y==0, 1], c='red', marker='o', s=100, \n",
    "                label='Class 0', alpha=0.8, edgecolors='black')\n",
    "    plt.scatter(X[y==1, 0], X[y==1, 1], c='blue', marker='s', s=100, \n",
    "                label='Class 1', alpha=0.8, edgecolors='black')\n",
    "    \n",
    "    # Highlight support vectors\n",
    "    support_vectors = model.support_vectors_\n",
    "    plt.scatter(support_vectors[:, 0], support_vectors[:, 1], \n",
    "                s=300, linewidth=3, facecolors='none', edgecolors='yellow',\n",
    "                label=f'Support Vectors ({len(support_vectors)})')\n",
    "    \n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    return plt\n",
    "\n",
    "# Train linear SVM\n",
    "svm_linear = svm.SVC(kernel='linear', C=1.0)\n",
    "svm_linear.fit(X, y)\n",
    "\n",
    "# Plot results\n",
    "plot_svm_decision_boundary(X, y, svm_linear, \n",
    "                          \"üéØ Linear SVM: The 'Street' Between Classes\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Linear SVM trained successfully!\")\n",
    "print(f\"üéØ Number of support vectors: {len(svm_linear.support_vectors_)}\")\n",
    "print(f\"üìè Training accuracy: {accuracy_score(y, svm_linear.predict(X)):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéõÔ∏è Part 3: The Impact of the C Parameter\n",
    "\n",
    "Let's see how the C parameter affects the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different C values\n",
    "C_values = [0.1, 1.0, 10.0, 100.0]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, C in enumerate(C_values):\n",
    "    # Train SVM with different C\n",
    "    svm_model = svm.SVC(kernel='linear', C=C)\n",
    "    svm_model.fit(X, y)\n",
    "    \n",
    "    # Plot on subplot\n",
    "    plt.sca(axes[i])\n",
    "    \n",
    "    # Create mesh\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Decision boundary\n",
    "    Z = svm_model.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot\n",
    "    plt.contour(xx, yy, Z, colors='black', levels=[-1, 0, 1], alpha=0.5,\n",
    "                linestyles=['--', '-', '--'])\n",
    "    plt.contourf(xx, yy, Z, levels=[-1, 1], colors=['lightcoral', 'lightblue'], alpha=0.3)\n",
    "    \n",
    "    plt.scatter(X[y==0, 0], X[y==0, 1], c='red', marker='o', s=50, alpha=0.8)\n",
    "    plt.scatter(X[y==1, 0], X[y==1, 1], c='blue', marker='s', s=50, alpha=0.8)\n",
    "    \n",
    "    # Support vectors\n",
    "    support_vectors = svm_model.support_vectors_\n",
    "    plt.scatter(support_vectors[:, 0], support_vectors[:, 1], \n",
    "                s=200, linewidth=2, facecolors='none', edgecolors='yellow')\n",
    "    \n",
    "    plt.title(f'C = {C}\\n({len(support_vectors)} support vectors)')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üéõÔ∏è Effect of C parameter:\")\n",
    "print(\"   üìâ Low C (0.1): Wider margin, more support vectors (less overfitting)\")\n",
    "print(\"   üìà High C (100): Narrower margin, fewer support vectors (more overfitting)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåÄ Part 4: Nonlinear Data - The Kernel Trick\n",
    "\n",
    "Let's create some nonlinear data and see how different kernels handle it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create nonlinear data (circles)\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "X_circles, y_circles = make_circles(n_samples=200, noise=0.2, factor=0.3, random_state=42)\n",
    "\n",
    "# Plot the circular data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_circles[y_circles==0, 0], X_circles[y_circles==0, 1], \n",
    "            c='red', marker='o', s=100, label='Class 0', alpha=0.7)\n",
    "plt.scatter(X_circles[y_circles==1, 0], X_circles[y_circles==1, 1], \n",
    "            c='blue', marker='s', s=100, label='Class 1', alpha=0.7)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('üåÄ Nonlinear Dataset (Circles)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "print(\"üåÄ Created circular dataset - linear boundaries won't work well here!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different kernels\n",
    "kernels = ['linear', 'poly', 'rbf']\n",
    "titles = ['Linear Kernel', 'Polynomial Kernel (degree=3)', 'RBF (Gaussian) Kernel']\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "for i, (kernel, title) in enumerate(zip(kernels, titles)):\n",
    "    # Train SVM with different kernel\n",
    "    if kernel == 'poly':\n",
    "        svm_model = svm.SVC(kernel=kernel, degree=3, C=1.0)\n",
    "    else:\n",
    "        svm_model = svm.SVC(kernel=kernel, C=1.0)\n",
    "    \n",
    "    svm_model.fit(X_circles, y_circles)\n",
    "    \n",
    "    # Plot\n",
    "    plt.sca(axes[i])\n",
    "    \n",
    "    # Create mesh\n",
    "    h = 0.02\n",
    "    x_min, x_max = X_circles[:, 0].min() - 0.5, X_circles[:, 0].max() + 0.5\n",
    "    y_min, y_max = X_circles[:, 1].min() - 0.5, X_circles[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Decision boundary\n",
    "    Z = svm_model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot decision regions\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, colors=['lightcoral', 'lightblue'])\n",
    "    \n",
    "    # Plot data\n",
    "    plt.scatter(X_circles[y_circles==0, 0], X_circles[y_circles==0, 1], \n",
    "                c='red', marker='o', s=50, alpha=0.8, edgecolors='black')\n",
    "    plt.scatter(X_circles[y_circles==1, 0], X_circles[y_circles==1, 1], \n",
    "                c='blue', marker='s', s=50, alpha=0.8, edgecolors='black')\n",
    "    \n",
    "    # Support vectors\n",
    "    support_vectors = svm_model.support_vectors_\n",
    "    plt.scatter(support_vectors[:, 0], support_vectors[:, 1], \n",
    "                s=200, linewidth=2, facecolors='none', edgecolors='yellow')\n",
    "    \n",
    "    accuracy = accuracy_score(y_circles, svm_model.predict(X_circles))\n",
    "    plt.title(f'{title}\\nAccuracy: {accuracy:.3f}')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üé™ Kernel Performance on Circular Data:\")\n",
    "print(\"   üìê Linear: Struggles with circular patterns\")\n",
    "print(\"   üî¢ Polynomial: Better, but can be complex\")\n",
    "print(\"   üåÄ RBF: Excellent for this type of nonlinear data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Part 5: Real-World Application - Iris Dataset\n",
    "\n",
    "Let's apply SVM to the classic Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X_iris = iris.data[:, :2]  # Use only first 2 features for visualization\n",
    "y_iris = iris.target\n",
    "\n",
    "# For simplicity, let's use only 2 classes\n",
    "mask = y_iris != 2  # Remove class 2 (Virginica)\n",
    "X_iris_2class = X_iris[mask]\n",
    "y_iris_2class = y_iris[mask]\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_iris_scaled = scaler.fit_transform(X_iris_2class)\n",
    "\n",
    "# Train SVM\n",
    "svm_iris = svm.SVC(kernel='rbf', C=1.0, gamma='scale')\n",
    "svm_iris.fit(X_iris_scaled, y_iris_2class)\n",
    "\n",
    "# Plot results\n",
    "plot_svm_decision_boundary(X_iris_scaled, y_iris_2class, svm_iris,\n",
    "                          \"üå∫ SVM on Iris Dataset (Setosa vs Versicolor)\")\n",
    "plt.xlabel('Sepal Length (scaled)')\n",
    "plt.ylabel('Sepal Width (scaled)')\n",
    "plt.show()\n",
    "\n",
    "# Print results\n",
    "accuracy = accuracy_score(y_iris_2class, svm_iris.predict(X_iris_scaled))\n",
    "print(f\"üå∫ Iris Dataset Results:\")\n",
    "print(f\"   üéØ Accuracy: {accuracy:.3f}\")\n",
    "print(f\"   üîó Support Vectors: {len(svm_iris.support_vectors_)}\")\n",
    "print(f\"   üìä Classes: Setosa (0) vs Versicolor (1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Summary and Key Takeaways\n",
    "\n",
    "### What we learned about Support Vector Machines:\n",
    "\n",
    "1. **üéØ Support Vectors**: Only the points closest to the decision boundary matter!\n",
    "2. **üìè Large Margin**: SVMs find the \"widest street\" between classes\n",
    "3. **üéõÔ∏è C Parameter**: Controls the trade-off between margin width and violations\n",
    "4. **üé™ Kernel Trick**: Enables nonlinear classification without explicit feature transformation\n",
    "5. **‚öñÔ∏è Feature Scaling**: Essential for SVM performance\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "- Experiment with different datasets\n",
    "- Try hyperparameter tuning\n",
    "- Explore SVM regression\n",
    "- Compare with other algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun experiment: Try changing parameters and see what happens!\n",
    "print(\"üéÆ Interactive Experiment Zone!\")\n",
    "print(\"=\" * 40)\n",
    "print(\"Try modifying these parameters in the cells above:\")\n",
    "print(\"üéõÔ∏è C values: 0.01, 0.1, 1, 10, 100, 1000\")\n",
    "print(\"üåÄ Gamma values: 0.001, 0.01, 0.1, 1, 10\")\n",
    "print(\"üî¢ Polynomial degrees: 2, 3, 4, 5\")\n",
    "print(\"\\nüí° Questions to explore:\")\n",
    "print(\"   ‚Ä¢ What happens with very high C?\")\n",
    "print(\"   ‚Ä¢ How does gamma affect RBF kernel boundaries?\")\n",
    "print(\"   ‚Ä¢ When does polynomial kernel work best?\")\n",
    "print(\"\\nüéØ Challenge: Can you find the optimal parameters for each dataset?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}